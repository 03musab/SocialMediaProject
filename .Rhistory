# ============================================
clean_text <- function(text) {
# Convert to lowercase
text <- tolower(text)
# Remove URLs
text <- str_remove_all(text, "http\\S+|www\\S+")
# Remove mentions (@username)
text <- str_remove_all(text, "@\\w+")
# Remove hashtags (but keep the word)
text <- str_remove_all(text, "#")
# Remove numbers
text <- str_remove_all(text, "\\d+")
# Remove punctuation
text <- str_remove_all(text, "[[:punct:]]")
# Remove extra whitespaces
text <- str_squish(text)
return(text)
}
# ============================================
# APPLY CLEANING
# ============================================
cat("\nüßπ Cleaning tweets...\n")
# Store original text
tweets$original_text <- tweets$text
# Clean text
tweets$cleaned_text <- sapply(tweets$text, clean_text)
# Show before/after examples
cat("\nüìù Before and After Cleaning:\n")
cat("\nOriginal 1:", tweets$original_text[1], "\n")
cat("Cleaned 1:", tweets$cleaned_text[1], "\n")
cat("\nOriginal 2:", tweets$original_text[5], "\n")
cat("Cleaned 2:", tweets$cleaned_text[5], "\n")
# ============================================
# TOKENIZATION & STOP WORDS REMOVAL
# ============================================
cat("\nüî§ Tokenizing and removing stop words...\n")
# Get stop words
data("stop_words")
# Tokenize tweets
tweets_tokens <- tweets %>%
select(tweet_id, cleaned_text) %>%
unnest_tokens(word, cleaned_text)
# Remove stop words
tweets_tokens <- tweets_tokens %>%
anti_join(stop_words, by = "word")
cat("‚úÖ Tokenization complete\n")
cat("Total words after cleaning:", nrow(tweets_tokens), "\n")
# ============================================
# WORD FREQUENCY ANALYSIS
# ============================================
cat("\nüìä Analyzing word frequencies...\n")
word_freq <- tweets_tokens %>%
count(word, sort = TRUE)
cat("\nüîù Top 20 Most Frequent Words:\n")
print(head(word_freq, 20))
# ============================================
# STEMMING (Optional but recommended)
# ============================================
cat("\nüåø Applying stemming...\n")
tweets_tokens$stemmed_word <- wordStem(tweets_tokens$word, language = "english")
# Word frequency after stemming
word_freq_stemmed <- tweets_tokens %>%
count(stemmed_word, sort = TRUE)
cat("\nüîù Top 20 Words After Stemming:\n")
print(head(word_freq_stemmed, 20))
# ============================================
# CREATE DOCUMENT-TERM MATRIX (for advanced analysis)
# ============================================
cat("\nüìã Creating Document-Term Matrix...\n")
# Create corpus
corpus <- Corpus(VectorSource(tweets$cleaned_text))
# Create DTM
dtm <- DocumentTermMatrix(corpus)
cat("DTM dimensions:", dim(dtm)[1], "documents x", dim(dtm)[2], "terms\n")
# Remove sparse terms (appear in < 1% of documents)
dtm_reduced <- removeSparseTerms(dtm, 0.99)
cat("Reduced DTM:", dim(dtm_reduced)[1], "documents x", dim(dtm_reduced)[2], "terms\n")
# ============================================
# SAVE CLEANED DATA
# ============================================
# Save cleaned tweets
write.csv(tweets, "tweets_cleaned.csv", row.names = FALSE)
cat("\nüíæ Saved cleaned tweets to 'tweets_cleaned.csv'\n")
# Save word frequencies
write.csv(word_freq, "word_frequencies.csv", row.names = FALSE)
cat("üíæ Saved word frequencies to 'word_frequencies.csv'\n")
# ============================================
# DATA QUALITY REPORT
# ============================================
cat("\n", paste(rep("=", 50), collapse = ""), "\n")
cat("üìä DATA QUALITY REPORT\n")
cat("=".rep(50), "\n")
# Data Preprocessing & Cleaning
library(dplyr)
library(stringr)
library(tm)
library(tidytext)
library(SnowballC)
# ============================================
# LOAD DATA
# ============================================
tweets <- read.csv("raw_tweets.csv", stringsAsFactors = FALSE)
cat("üìä Loaded", nrow(tweets), "tweets\n")
# ============================================
# TEXT CLEANING FUNCTIONS
# ============================================
clean_text <- function(text) {
# Convert to lowercase
text <- tolower(text)
# Remove URLs
text <- str_remove_all(text, "http\\S+|www\\S+")
# Remove mentions (@username)
text <- str_remove_all(text, "@\\w+")
# Remove hashtags (but keep the word)
text <- str_remove_all(text, "#")
# Remove numbers
text <- str_remove_all(text, "\\d+")
# Remove punctuation
text <- str_remove_all(text, "[[:punct:]]")
# Remove extra whitespaces
text <- str_squish(text)
return(text)
}
# ============================================
# APPLY CLEANING
# ============================================
cat("\nüßπ Cleaning tweets...\n")
# Store original text
tweets$original_text <- tweets$text
# Clean text
tweets$cleaned_text <- sapply(tweets$text, clean_text)
# Show before/after examples
cat("\nüìù Before and After Cleaning:\n")
cat("\nOriginal 1:", tweets$original_text[1], "\n")
cat("Cleaned 1:", tweets$cleaned_text[1], "\n")
cat("\nOriginal 2:", tweets$original_text[5], "\n")
cat("Cleaned 2:", tweets$cleaned_text[5], "\n")
# ============================================
# TOKENIZATION & STOP WORDS REMOVAL
# ============================================
cat("\nüî§ Tokenizing and removing stop words...\n")
# Get stop words
data("stop_words")
# Tokenize tweets
tweets_tokens <- tweets %>%
select(tweet_id, cleaned_text) %>%
unnest_tokens(word, cleaned_text)
# Remove stop words
tweets_tokens <- tweets_tokens %>%
anti_join(stop_words, by = "word")
cat("‚úÖ Tokenization complete\n")
cat("Total words after cleaning:", nrow(tweets_tokens), "\n")
# ============================================
# WORD FREQUENCY ANALYSIS
# ============================================
cat("\nüìä Analyzing word frequencies...\n")
word_freq <- tweets_tokens %>%
count(word, sort = TRUE)
cat("\nüîù Top 20 Most Frequent Words:\n")
print(head(word_freq, 20))
# ============================================
# STEMMING (Optional but recommended)
# ============================================
cat("\nüåø Applying stemming...\n")
tweets_tokens$stemmed_word <- wordStem(tweets_tokens$word, language = "english")
# Word frequency after stemming
word_freq_stemmed <- tweets_tokens %>%
count(stemmed_word, sort = TRUE)
cat("\nüîù Top 20 Words After Stemming:\n")
print(head(word_freq_stemmed, 20))
# ============================================
# CREATE DOCUMENT-TERM MATRIX (for advanced analysis)
# ============================================
cat("\nüìã Creating Document-Term Matrix...\n")
# Create corpus
corpus <- Corpus(VectorSource(tweets$cleaned_text))
# Create DTM
dtm <- DocumentTermMatrix(corpus)
cat("DTM dimensions:", dim(dtm)[1], "documents x", dim(dtm)[2], "terms\n")
# Remove sparse terms (appear in < 1% of documents)
dtm_reduced <- removeSparseTerms(dtm, 0.99)
cat("Reduced DTM:", dim(dtm_reduced)[1], "documents x", dim(dtm_reduced)[2], "terms\n")
# ============================================
# SAVE CLEANED DATA
# ============================================
# Save cleaned tweets
write.csv(tweets, "tweets_cleaned.csv", row.names = FALSE)
cat("\nüíæ Saved cleaned tweets to 'tweets_cleaned.csv'\n")
# Save word frequencies
write.csv(word_freq, "word_frequencies.csv", row.names = FALSE)
cat("üíæ Saved word frequencies to 'word_frequencies.csv'\n")
# ============================================
# DATA QUALITY REPORT
# ============================================
cat("\n", paste(rep("=", 50), collapse = ""), "\n")
cat("üìä DATA QUALITY REPORT\n")
cat("\n", paste(rep("=", 50), collapse = ""), "\n")
cat("\n‚úÖ Original tweets:", nrow(tweets), "\n")
cat("‚úÖ Tweets after cleaning:", nrow(tweets), "\n")
cat("‚úÖ Unique words:", nrow(word_freq), "\n")
cat("‚úÖ Average words per tweet:",
round(nrow(tweets_tokens) / nrow(tweets), 2), "\n")
# Check for empty tweets after cleaning
empty_tweets <- sum(tweets$cleaned_text == "")
cat("‚ö†Ô∏è  Empty tweets after cleaning:", empty_tweets, "\n")
# Remove empty tweets if any
if(empty_tweets > 0) {
tweets <- tweets %>% filter(cleaned_text != "")
cat("üßπ Removed", empty_tweets, "empty tweets\n")
write.csv(tweets, "tweets_cleaned.csv", row.names = FALSE)
}
cat("\n‚úÖ Preprocessing completed successfully!\n")
# MapReduce Simulation in R
library(dplyr)
library(parallel)
library(foreach)
install.packages("foreach")
# MapReduce Simulation in R
library(dplyr)
library(parallel)
library(foreach)
library(doParallel)
install.packages("doParallel")
# MapReduce Simulation in R
library(dplyr)
library(parallel)
library(foreach)
library(doParallel)
# ============================================
# LOAD CLEANED DATA
# ============================================
tweets <- read.csv("tweets_cleaned.csv", stringsAsFactors = FALSE)
cat("üìä Loaded", nrow(tweets), "cleaned tweets\n")
# ============================================
# SETUP PARALLEL PROCESSING
# ============================================
# Detect number of cores
n_cores <- detectCores() - 1  # Leave one core free
cat("üíª Using", n_cores, "CPU cores for parallel processing\n")
# Register parallel backend
cl <- makeCluster(n_cores)
registerDoParallel(cl)
cat("‚úÖ Parallel processing setup complete\n\n")
# ============================================
# MAPREDUCE 1: WORD COUNT
# ============================================
cat("=" , rep("=", 50), "\n")
cat("MAPREDUCE 1: WORD COUNT\n")
cat("=", rep("=", 50), "\n\n")
# MAP PHASE: Split data and count words in each chunk
map_word_count <- function(text_chunk) {
# Split text into words
words <- unlist(strsplit(text_chunk, " "))
# Count each word
word_counts <- table(words)
# Return as data frame
data.frame(
word = names(word_counts),
count = as.numeric(word_counts),
stringsAsFactors = FALSE
)
}
cat("üó∫Ô∏è  MAP Phase: Processing tweets in parallel...\n")
start_time <- Sys.time()
# Split tweets into chunks for parallel processing
chunk_size <- ceiling(nrow(tweets) / n_cores)
tweet_chunks <- split(tweets$cleaned_text,
ceiling(seq_along(tweets$cleaned_text) / chunk_size))
# Apply MAP function in parallel
map_results <- foreach(chunk = tweet_chunks, .combine = rbind,
.packages = c("dplyr")) %dopar% {
# Process each chunk
all_words <- unlist(strsplit(chunk, " "))
word_table <- table(all_words)
data.frame(word = names(word_table),
count = as.numeric(word_table),
stringsAsFactors = FALSE)
}
map_time <- Sys.time() - start_time
cat("‚úÖ MAP Phase completed in", round(map_time, 2), "seconds\n")
# REDUCE PHASE: Aggregate counts across all chunks
cat("\nüîÑ REDUCE Phase: Aggregating results...\n")
start_time <- Sys.time()
word_count_final <- map_results %>%
group_by(word) %>%
summarise(total_count = sum(count)) %>%
arrange(desc(total_count))
reduce_time <- Sys.time() - start_time
cat("‚úÖ REDUCE Phase completed in", round(reduce_time, 2), "seconds\n")
cat("\nüîù Top 15 Words:\n")
print(head(word_count_final, 15))
# ============================================
# MAPREDUCE 2: SENTIMENT AGGREGATION BY DATE
# ============================================
cat("\n\n", "=", rep("=", 50), "\n")
cat("MAPREDUCE 2: SENTIMENT AGGREGATION BY DATE\n")
cat("=", rep("=", 50), "\n\n")
# Load sentiment lexicon (simple positive/negative words)
positive_words <- c("love", "amazing", "best", "great", "excellent",
"fantastic", "recommend", "worth", "outstanding", "good",
"wonderful", "perfect", "happy", "awesome", "brilliant")
negative_words <- c("disappointed", "worst", "poor", "terrible", "bad",
"waste", "frustrating", "broke", "regret", "awful",
"horrible", "useless", "pathetic", "disappointed", "hate")
# MAP PHASE: Calculate sentiment for each tweet
map_sentiment <- function(text_chunk, dates_chunk) {
sentiments <- sapply(text_chunk, function(text) {
words <- unlist(strsplit(text, " "))
pos_count <- sum(words %in% positive_words)
neg_count <- sum(words %in% negative_words)
pos_count - neg_count  # Simple sentiment score
})
data.frame(
date = as.Date(dates_chunk),
sentiment = sentiments,
stringsAsFactors = FALSE
)
}
cat("üó∫Ô∏è  MAP Phase: Calculating sentiment scores...\n")
start_time <- Sys.time()
# Convert timestamp to date
tweets$date <- as.Date(tweets$timestamp)
# Split data into chunks
date_chunks <- split(tweets$date,
ceiling(seq_along(tweets$date) / chunk_size))
text_chunks <- split(tweets$cleaned_text,
ceiling(seq_along(tweets$cleaned_text) / chunk_size))
# Apply MAP in parallel
sentiment_map_results <- foreach(i = 1:length(text_chunks),
.combine = rbind,
.packages = c("dplyr")) %dopar% {
map_sentiment(text_chunks[[i]], date_chunks[[i]])
}
map_time <- Sys.time() - start_time
cat("‚úÖ MAP Phase completed in", round(map_time, 2), "seconds\n")
# REDUCE PHASE: Aggregate sentiment by date
cat("\nüîÑ REDUCE Phase: Aggregating sentiment by date...\n")
start_time <- Sys.time()
daily_sentiment <- sentiment_map_results %>%
group_by(date) %>%
summarise(
avg_sentiment = mean(sentiment),
total_tweets = n(),
positive_tweets = sum(sentiment > 0),
negative_tweets = sum(sentiment < 0),
neutral_tweets = sum(sentiment == 0)
) %>%
arrange(date)
reduce_time <- Sys.time() - start_time
cat("‚úÖ REDUCE Phase completed in", round(reduce_time, 2), "seconds\n")
cat("\nüìä Daily Sentiment Summary (First 10 days):\n")
print(head(daily_sentiment, 10))
# ============================================
# MAPREDUCE 3: ENGAGEMENT ANALYSIS BY USER
# ============================================
cat("\n\n", "=", rep("=", 50), "\n")
cat("MAPREDUCE 3: USER ENGAGEMENT ANALYSIS\n")
cat("=", rep("=", 50), "\n\n")
# MAP PHASE: Calculate engagement per user
map_user_engagement <- function(user_chunk, likes_chunk, retweets_chunk) {
data.frame(
user = user_chunk,
likes = likes_chunk,
retweets = retweets_chunk,
engagement = likes_chunk + retweets_chunk,
stringsAsFactors = FALSE
)
}
cat("üó∫Ô∏è  MAP Phase: Calculating user engagement...\n")
user_chunks <- split(tweets$user_id,
ceiling(seq_along(tweets$user_id) / chunk_size))
likes_chunks <- split(tweets$likes,
ceiling(seq_along(tweets$likes) / chunk_size))
retweets_chunks <- split(tweets$retweets,
ceiling(seq_along(tweets$retweets) / chunk_size))
engagement_map <- foreach(i = 1:length(user_chunks),
.combine = rbind) %dopar% {
map_user_engagement(user_chunks[[i]], likes_chunks[[i]], retweets_chunks[[i]])
}
# REDUCE PHASE: Aggregate by user
cat("üîÑ REDUCE Phase: Aggregating user stats...\n")
user_stats <- engagement_map %>%
group_by(user) %>%
summarise(
total_tweets = n(),
total_likes = sum(likes),
total_retweets = sum(retweets),
total_engagement = sum(engagement),
avg_engagement = mean(engagement)
) %>%
arrange(desc(total_engagement))
cat("\nüèÜ Top 10 Most Engaged Users:\n")
print(head(user_stats, 10))
# ============================================
# SAVE RESULTS
# ============================================
cat("\nüíæ Saving MapReduce results...\n")
write.csv(word_count_final, "mapreduce_word_count.csv", row.names = FALSE)
write.csv(daily_sentiment, "mapreduce_daily_sentiment.csv", row.names = FALSE)
write.csv(user_stats, "mapreduce_user_stats.csv", row.names = FALSE)
cat("‚úÖ Saved word counts to 'mapreduce_word_count.csv'\n")
cat("‚úÖ Saved daily sentiment to 'mapreduce_daily_sentiment.csv'\n")
cat("‚úÖ Saved user stats to 'mapreduce_user_stats.csv'\n")
# ============================================
# PERFORMANCE SUMMARY
# ============================================
cat("\n", "=", rep("=", 50), "\n")
cat("‚ö° PERFORMANCE SUMMARY\n")
cat("=", rep("=", 50), "\n\n")
cat("CPU Cores Used:", n_cores, "\n")
cat("Total Tweets Processed:", nrow(tweets), "\n")
cat("Unique Words Found:", nrow(word_count_final), "\n")
cat("Date Range:", nrow(daily_sentiment), "days\n")
cat("Unique Users:", nrow(user_stats), "\n")
# Stop parallel cluster
stopCluster(cl)
cat("\n‚úÖ MapReduce simulation completed successfully!\n")
# Advanced Sentiment Analysis
library(dplyr)
library(syuzhet)
install
install.packages("syuzhet")
# Advanced Sentiment Analysis
library(dplyr)
library(syuzhet)
library(tidytext)
library(ggplot2)
install
install.packages("ggplot2")
# Advanced Sentiment Analysis
library(dplyr)
library(syuzhet)
library(tidytext)
library(ggplot2)
# ============================================
# LOAD DATA
# ============================================
tweets <- read.csv("tweets_cleaned.csv", stringsAsFactors = FALSE)
cat("üìä Loaded", nrow(tweets), "tweets for sentiment analysis\n\n")
# ============================================
# METHOD 1: AFINN LEXICON (Score-based)
# ============================================
cat("=" , rep("=", 50), "\n")
cat("METHOD 1: AFINN SENTIMENT SCORING\n")
cat("=", rep("=", 50), "\n\n")
# Get AFINN sentiment scores (-5 to +5)
tweets$sentiment_afinn <- get_sentiment(tweets$cleaned_text, method = "afinn")
# Classify sentiment
tweets$sentiment_class_afinn <- ifelse(tweets$sentiment_afinn > 0, "Positive",
ifelse(tweets$sentiment_afinn < 0, "Negative",
"Neutral"))
# Summary
cat("üìä AFINN Sentiment Distribution:\n")
table(tweets$sentiment_class_afinn) %>% print()
cat("\nüìà Average Sentiment Score:",
round(mean(tweets$sentiment_afinn), 3), "\n")
# ============================================
# METHOD 2: BING LEXICON (Binary)
# ============================================
cat("\n\n", "=", rep("=", 50), "\n")
cat("METHOD 2: BING SENTIMENT (Positive/Negative)\n")
cat("=", rep("=", 50), "\n\n")
# Get Bing sentiment
tweets$sentiment_bing <- get_sentiment(tweets$cleaned_text, method = "bing")
tweets$sentiment_class_bing <- ifelse(tweets$sentiment_bing > 0, "Positive",
ifelse(tweets$sentiment_bing < 0, "Negative",
"Neutral"))
cat("üìä BING Sentiment Distribution:\n")
table(tweets$sentiment_class_bing) %>% print()
# ============================================
# METHOD 3: NRC EMOTION LEXICON
# ============================================
cat("\n\n", "=", rep("=", 50), "\n")
cat("METHOD 3: NRC EMOTION ANALYSIS\n")
cat("=", rep("=", 50), "\n\n")
# Get NRC emotions for first 1000 tweets (faster processing)
sample_tweets <- tweets$cleaned_text[1:min(1000, nrow(tweets))]
cat("üîç Analyzing emotions for", length(sample_tweets), "tweets...\n")
emotions <- get_nrc_sentiment(sample_tweets)
# Add emotions to tweets
tweets_with_emotions <- cbind(tweets[1:length(sample_tweets), ], emotions)
# Sum of each emotion
emotion_totals <- colSums(emotions)
cat("\nüòä Total Emotions Detected:\n")
print(emotion_totals)
# Most dominant emotion per tweet
tweets_with_emotions$dominant_emotion <- apply(emotions, 1, function(x) {
if(sum(x) == 0) return("None")
names(which.max(x))
})
cat("\nüé≠ Most Common Emotions:\n")
table(tweets_with_emotions$dominant_emotion) %>%
sort(decreasing = TRUE) %>%
head(10) %>%
print()
# ============================================
# METHOD 4: SYUZHET METHOD
# ============================================
cat("\n\n", "=", rep("=", 50), "\n")
cat("METHOD
# Create project folders
dir.create("SocialMediaProject")
# Create project folders
dir.create("SocialMediaProject")
setwd("SocialMediaProject")
# Create subfolders
dir.create("data")
dir.create("scripts")
dir.create("output")
dir.create("plots")
cat("‚úÖ Project structure created!\n")
getwd()  # Shows your current folder location
